{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPY0J1K5Rux5olqjFaP3R+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n","%load_ext nvcc_plugin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6ykPW50safe","executionInfo":{"status":"ok","timestamp":1701355612652,"user_tz":-330,"elapsed":10172,"user":{"displayName":"Venkatesh Elangovan","userId":"12247996726828990634"}},"outputId":"8a582ff3-9407-4f06-9497-8b531b9907ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n","  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-co1_l9l5\n","  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-co1_l9l5\n","  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: NVCCPlugin\n","  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4294 sha256=abca94b94e17400bf7274068b0d7fd96773cd4ce6ebac27fd8b14a8dafe8d57c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-e161hq89/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n","Successfully built NVCCPlugin\n","Installing collected packages: NVCCPlugin\n","Successfully installed NVCCPlugin-0.0.2\n","created output directory at /content/src\n","Out bin /content/result.out\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGzTFTtwsXul","executionInfo":{"status":"ok","timestamp":1701355731374,"user_tz":-330,"elapsed":4659,"user":{"displayName":"Venkatesh Elangovan","userId":"12247996726828990634"}},"outputId":"53c3b301-ab0c-41c8-fce9-e49544703195"},"outputs":[{"output_type":"stream","name":"stdout","text":["/tmp/tmpjunwf17g/75b66920-02b8-431d-9996-5cca985f0216.out starting transpose at device 0: Tesla T4  with matrix nrows 4096 ncols 4096\n","copyGmem elapsed 0.001288 sec <<< grid (256,256) block (16,16)>>> effective bandwidth 104.211395 GB\n","naiveGmem elapsed 0.001253 sec <<< grid (256,256) block (16,16)>>> effective bandwidth 107.126534 GB\n","naiveGmemUnroll elapsed 0.001177 sec <<< grid (128,256) block (16,16)>>> effective bandwidth 114.026726 GB\n","transposeSmem elapsed 0.000950 sec <<< grid (256,256) block (16,16)>>> effective bandwidth 141.302704 GB\n","transposeSmemPad elapsed 0.001429 sec <<< grid (256,256) block (16,16)>>> effective bandwidth 93.918915 GB\n","transposeSmemDyn elapsed 0.001322 sec <<< grid (256,256) block (16,16)>>> effective bandwidth 101.523888 GB\n","transposeSmemPadDyn elapsed 0.001372 sec <<< grid (256,256) block (16,16)>>> effective bandwidth 97.819283 GB\n","transposeSmemUnroll elapsed 0.001193 sec <<< grid (128,256) block (16,16)>>> effective bandwidth 112.499992 GB\n","transposeSmemUnrollPad elapsed 0.001200 sec <<< grid (128,256) block (16,16)>>> effective bandwidth 111.851768 GB\n","transposeSmemUnrollPadDyn elapsed 0.001193 sec <<< grid (128,256) block (16,16)>>> effective bandwidth 112.499992 GB\n","\n"]}],"source":["%%cu\n","\n","\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","\n","#include <cuda.h>\n","#include <sys/time.h>\n","#ifndef _COMMON_H\n","#define _COMMON_H\n","\n","#define CHECK(call)                                                            \\\n","{                                                                              \\\n","    const cudaError_t error = call;                                            \\\n","    if (error != cudaSuccess)                                                  \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n","        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n","                cudaGetErrorString(error));                                    \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUBLAS(call)                                                     \\\n","{                                                                              \\\n","    cublasStatus_t err;                                                        \\\n","    if ((err = (call)) != CUBLAS_STATUS_SUCCESS)                               \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CUBLAS error %d at %s:%d\\n\", err, __FILE__,       \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CURAND(call)                                                     \\\n","{                                                                              \\\n","    curandStatus_t err;                                                        \\\n","    if ((err = (call)) != CURAND_STATUS_SUCCESS)                               \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CURAND error %d at %s:%d\\n\", err, __FILE__,       \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUFFT(call)                                                      \\\n","{                                                                              \\\n","    cufftResult err;                                                           \\\n","    if ( (err = (call)) != CUFFT_SUCCESS)                                      \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CUFFT error %d at %s:%d\\n\", err, __FILE__,        \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUSPARSE(call)                                                   \\\n","{                                                                              \\\n","    cusparseStatus_t err;                                                      \\\n","    if ((err = (call)) != CUSPARSE_STATUS_SUCCESS)                             \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got error %d at %s:%d\\n\", err, __FILE__, __LINE__);   \\\n","        cudaError_t cuda_err = cudaGetLastError();                             \\\n","        if (cuda_err != cudaSuccess)                                           \\\n","        {                                                                      \\\n","            fprintf(stderr, \"  CUDA error \\\"%s\\\" also detected\\n\",             \\\n","                    cudaGetErrorString(cuda_err));                             \\\n","        }                                                                      \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","inline double seconds()\n","{\n","    struct timeval tp;\n","    struct timezone tzp;\n","    int i = gettimeofday(&tp, &tzp);\n","    return ((double)tp.tv_sec + (double)tp.tv_usec * 1.e-6);\n","}\n","\n","#endif // _COMMON_H\n","\n","/*\n"," * Example kernels for transposing a rectangular host array using a variety of\n"," * optimizations, including shared memory, unrolling, and memory padding.\n"," */\n","\n","// Some kernels assume square blocks\n","#define BDIMX 16\n","#define BDIMY BDIMX\n","\n","#define INDEX(ROW, COL, INNER) ((ROW) * (INNER) + (COL))\n","\n","#define IPAD 2\n","\n","void initialData(float *in,  const int size)\n","{\n","    for (int i = 0; i < size; i++)\n","    {\n","        in[i] = (float)(rand() & 0xFF) / 10.0f;\n","    }\n","\n","    return;\n","}\n","\n","void printData(float *in,  const int size)\n","{\n","    for (int i = 0; i < size; i++)\n","    {\n","        printf(\"%3.0f \", in[i]);\n","    }\n","\n","    printf(\"\\n\");\n","    return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, int rows, int cols)\n","{\n","    double epsilon = 1.0E-8;\n","    bool match = 1;\n","\n","    for (int i = 0; i < rows; i++)\n","    {\n","        for (int j = 0; j < cols; j++)\n","        {\n","            int index = INDEX(i, j, cols);\n","            if (abs(hostRef[index] - gpuRef[index]) > epsilon) {\n","                match = 0;\n","                printf(\"different on (%d, %d) (offset=%d) element in \"\n","                        \"transposed matrix: host %f gpu %f\\n\", i, j, index,\n","                        hostRef[index], gpuRef[index]);\n","                break;\n","            }\n","        }\n","        if (!match) break;\n","    }\n","\n","    if (!match)  printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","void transposeHost(float *out, float *in, const int nrows, const int ncols)\n","{\n","    for (int iy = 0; iy < nrows; ++iy)\n","    {\n","        for (int ix = 0; ix < ncols; ++ix)\n","        {\n","            out[INDEX(ix, iy, nrows)] = in[INDEX(iy, ix, ncols)];\n","        }\n","    }\n","}\n","\n","__global__ void copyGmem(float *out, float *in, const int nrows, const int ncols)\n","{\n","    // matrix coordinate (ix,iy)\n","    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    unsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    // transpose with boundary test\n","    if (row < nrows && col < ncols)\n","    {\n","\t\t    // NOTE this is a transpose, not a copy\n","        out[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","    }\n","}\n","\n","__global__ void naiveGmem(float *out, float *in, const int nrows, const int ncols)\n","{\n","    // matrix coordinate (ix,iy)\n","    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    unsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    // transpose with boundary test\n","    if (row < nrows && col < ncols)\n","    {\n","        out[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","    }\n","}\n","\n","__global__ void naiveGmemUnroll(float *out, float *in, const int nrows,\n","                                const int ncols)\n","{\n","    // Pretend there are twice as many blocks in the x direction\n","    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    unsigned int col = (2 * blockIdx.x * blockDim.x) + threadIdx.x;\n","\n","    if (row < nrows)\n","    {\n","        if (col < ncols)\n","        {\n","            out[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","        }\n","\n","        col += blockDim.x;\n","\n","        if (col < ncols)\n","        {\n","            out[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","        }\n","    }\n","}\n","\n","__global__ void transposeSmem(float *out, float *in, int nrows, int ncols)\n","{\n","    // static shared memory\n","    __shared__ float tile[BDIMY][BDIMX];\n","\n","    // coordinate in original matrix\n","    unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","    unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","\n","    if (row < nrows && col < ncols)\n","    {\n","      // load data from global memory to shared memory\n","      tile[threadIdx.y][threadIdx.x] = in[offset];\n","    }\n","\n","    // thread index in transposed block\n","    unsigned int bidx, irow, icol;\n","    bidx = threadIdx.y * blockDim.x + threadIdx.x;\n","    irow = bidx / blockDim.y;\n","    icol = bidx % blockDim.y;\n","\n","\t  // NOTE - need to transpose row and col on block and thread-block level:\n","\t  // 1. swap blocks x-y\n","\t  // 2. swap thread x-y assignment (irow and icol calculations above)\n","\t  // note col still has continuous threadIdx.x -> coalesced gst\n","\t  col = blockIdx.y * blockDim.y + icol;\n","\t  row = blockIdx.x * blockDim.x + irow;\n","\n","    // linear global memory index for transposed matrix\n","\t  // NOTE nrows is stride of result, row and col are transposed\n","    unsigned int transposed_offset = INDEX(row, col, nrows);\n","    // thread synchronization\n","    __syncthreads();\n","\n","\t  // NOTE invert sizes for write check\n","    if (row < ncols && col < nrows)\n","    {\n","        // store data to global memory from shared memory\n","        out[transposed_offset] = tile[icol][irow]; // NOTE icol,irow not irow,icol\n","    }\n","}\n","\n","__global__ void transposeSmemUnroll(float *out, float *in, const int nrows,\n","                                            const int ncols)\n","{\n","    // static 1D shared memory\n","    __shared__ float tile[BDIMY][BDIMX * 2];\n","\n","    // coordinate in original matrix\n","    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    unsigned int col = (2 * blockIdx.x * blockDim.x) + threadIdx.x;\n","\n","    unsigned int row2 = row;\n","    unsigned int col2 = col + blockDim.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","    unsigned int offset2 = INDEX(row2, col2, ncols);\n","\n","    // thread index in transposed block\n","    unsigned int bidx = threadIdx.y * blockDim.x + threadIdx.x;\n","    unsigned int irow = bidx / blockDim.y;\n","    unsigned int icol = bidx % blockDim.y;\n","\n","    // linear global memory index for transposed matrix\n","    unsigned int transposed_offset = INDEX(col, row, nrows);\n","    unsigned int transposed_offset2 = INDEX(col2, row2, nrows);\n","\n","    if (row < nrows && col < ncols)\n","    {\n","        tile[threadIdx.y][threadIdx.x] = in[offset];\n","    }\n","    if (row2 < nrows && col2 < ncols)\n","    {\n","        tile[threadIdx.y][blockDim.x + threadIdx.x] = in[offset2];\n","    }\n","\n","    __syncthreads();\n","\n","    if (row < nrows && col < ncols)\n","    {\n","        out[transposed_offset] = tile[irow][icol];\n","    }\n","    if (row2 < nrows && col2 < ncols)\n","    {\n","        out[transposed_offset2] = tile[irow][blockDim.x + icol];\n","    }\n","}\n","\n","__global__ void transposeSmemUnrollPad(float *out, float *in, const int nrows,\n","                                       const int ncols)\n","{\n","    // static 1D shared memory with padding\n","    __shared__ float tile[BDIMY][BDIMX * 2 + IPAD];\n","\n","    // coordinate in original matrix\n","    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    unsigned int col = (2 * blockIdx.x * blockDim.x) + threadIdx.x;\n","\n","    unsigned int row2 = row;\n","    unsigned int col2 = col + blockDim.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","    unsigned int offset2 = INDEX(row2, col2, ncols);\n","\n","    // thread index in transposed block\n","    unsigned int bidx = threadIdx.y * blockDim.x + threadIdx.x;\n","    unsigned int irow = bidx / blockDim.y;\n","    unsigned int icol = bidx % blockDim.y;\n","\n","    // linear global memory index for transposed matrix\n","    unsigned int transposed_offset = INDEX(col, row, nrows);\n","    unsigned int transposed_offset2 = INDEX(col2, row2, nrows);\n","\n","    if (row < nrows && col < ncols)\n","    {\n","        tile[threadIdx.y][threadIdx.x] = in[offset];\n","    }\n","    if (row2 < nrows && col2 < ncols)\n","    {\n","        tile[threadIdx.y][blockDim.x + threadIdx.x] = in[offset2];\n","    }\n","\n","    __syncthreads();\n","\n","    if (row < nrows && col < ncols)\n","    {\n","        out[transposed_offset] = tile[irow][icol];\n","    }\n","    if (row2 < nrows && col2 < ncols)\n","    {\n","        out[transposed_offset2] = tile[irow][blockDim.x + icol];\n","    }\n","}\n","\n","__global__ void transposeSmemUnrollPadDyn (float *out, float *in, const int nrows,\n","        const int ncols)\n","{\n","    // dynamic shared memory\n","    extern __shared__ float tile[];\n","\n","    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    unsigned int col = (2 * blockIdx.x * blockDim.x) + threadIdx.x;\n","\n","    unsigned int row2 = row;\n","    unsigned int col2 = col + blockDim.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","    unsigned int offset2 = INDEX(row2, col2, ncols);\n","\n","    // thread index in transposed block\n","    unsigned int bidx = threadIdx.y * blockDim.x + threadIdx.x;\n","    unsigned int irow = bidx / blockDim.y;\n","    unsigned int icol = bidx % blockDim.y;\n","\n","    // coordinate in transposed matrix\n","    unsigned int transposed_offset = INDEX(col, row, nrows);\n","    unsigned int transposed_offset2 = INDEX(col2, row2, nrows);\n","\n","    if (row < nrows && col < ncols)\n","    {\n","        tile[INDEX(threadIdx.y, threadIdx.x, BDIMX * 2 + IPAD)] = in[offset];\n","    }\n","    if (row2 < nrows && col2 < ncols)\n","    {\n","        tile[INDEX(threadIdx.y, blockDim.x + threadIdx.x, BDIMX * 2 + IPAD)] =\n","            in[offset2];\n","    }\n","\n","    __syncthreads();\n","\n","    if (row < nrows && col < ncols)\n","    {\n","        out[transposed_offset] = tile[INDEX(irow, icol, BDIMX * 2 + IPAD)];\n","    }\n","    if (row2 < nrows && col2 < ncols)\n","    {\n","        out[transposed_offset2] = tile[INDEX(irow, blockDim.x + icol, BDIMX * 2 + IPAD)];\n","    }\n","}\n","\n","__global__ void transposeSmemPad(float *out, float *in, int nrows, int ncols)\n","{\n","    // static shared memory with padding\n","    __shared__ float tile[BDIMY][BDIMX + IPAD];\n","\n","    // coordinate in original matrix\n","    unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","    unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","\n","    // thread index in transposed block\n","    unsigned int bidx, irow, icol;\n","    bidx = threadIdx.y * blockDim.x + threadIdx.x;\n","    irow = bidx / blockDim.y;\n","    icol = bidx % blockDim.y;\n","\n","    // linear global memory index for transposed matrix\n","    unsigned int transposed_offset = INDEX(col, row, nrows);\n","\n","    // transpose with boundary test\n","    if (row < nrows && col < ncols)\n","    {\n","        // load data from global memory to shared memory\n","        tile[threadIdx.y][threadIdx.x] = in[offset];\n","\n","        // thread synchronization\n","        __syncthreads();\n","\n","        // store data to global memory from shared memory\n","        out[transposed_offset] = tile[irow][icol];\n","    }\n","}\n","\n","__global__ void transposeSmemDyn(float *out, float *in, int nrows, int ncols)\n","{\n","    // dynamic shared memory\n","    extern __shared__ float tile[];\n","\n","    // coordinate in original matrix\n","    unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","    unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","\n","    // thread index in transposed block\n","    unsigned int row_idx, col_idx, irow, icol;\n","    row_idx = threadIdx.y * blockDim.x + threadIdx.x;\n","    irow    = row_idx / blockDim.y;\n","    icol    = row_idx % blockDim.y;\n","    col_idx = irow * blockDim.x + icol;\n","\n","    // linear global memory index for transposed matrix\n","    unsigned int transposed_offset = INDEX(col, row, nrows);\n","\n","    // transpose with boundary test\n","    if (row < nrows && col < ncols)\n","    {\n","        // load data from global memory to shared memory\n","        tile[row_idx] = in[offset];\n","\n","        // thread synchronization\n","        __syncthreads();\n","\n","        // store data to global memory from shared memory\n","        out[transposed_offset] = tile[col_idx];\n","    }\n","}\n","\n","__global__ void transposeSmemPadDyn(float *out, float *in, int nrows, int ncols)\n","{\n","    // static shared memory with padding\n","    extern __shared__ float tile[];\n","\n","    // coordinate in original matrix\n","    unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","    unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","    // linear global memory index for original matrix\n","    unsigned int offset = INDEX(row, col, ncols);\n","\n","    // thread index in transposed block\n","    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;\n","    unsigned int row_idx = threadIdx.y * (blockDim.x + IPAD) + threadIdx.x;\n","    unsigned int irow    = idx / blockDim.y;\n","    unsigned int icol    = idx % blockDim.y;\n","    unsigned int col_idx = irow * (blockDim.x + IPAD) + icol;\n","\n","    // linear global memory index for transposed matrix\n","    unsigned int transposed_offset = INDEX(col, row, nrows);\n","\n","    // transpose with boundary test\n","    if (row < nrows && col < ncols)\n","    {\n","        // load data from global memory to shared memory\n","        tile[row_idx] = in[offset];\n","\n","        // thread synchronization\n","        __syncthreads();\n","\n","        // store data to global memory from shared memory\n","        out[transposed_offset] = tile[col_idx];\n","    }\n","}\n","\n","int main(int argc, char **argv)\n","{\n","    // set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"%s starting transpose at \", argv[0]);\n","    printf(\"device %d: %s \", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","\n","    bool iprint = 0;\n","\n","    // set up array size 2048\n","    int nrows = 1 << 12;\n","    int ncols = 1 << 12;\n","\n","    if (argc > 1) iprint = atoi(argv[1]);\n","\n","    if (argc > 2) nrows = atoi(argv[2]);\n","\n","    if (argc > 3) ncols = atoi(argv[3]);\n","\n","    printf(\" with matrix nrows %d ncols %d\\n\", nrows, ncols);\n","    size_t ncells = nrows * ncols;\n","    size_t nBytes = ncells * sizeof(float);\n","\n","    // execution configuration\n","    dim3 block (BDIMX, BDIMY);\n","    /*\n","     * Map CUDA blocks/threads to output space. Map rows in output to same\n","     * x-value in CUDA, columns to same y-value.\n","     */\n","    dim3 grid ((ncols + block.x - 1) / block.x, (nrows + block.y - 1) / block.y);\n","    dim3 grid2 ((grid.x + 2 - 1) / 2, grid.y);\n","\n","    // allocate host memory\n","    float *h_A = (float *)malloc(nBytes);\n","    float *hostRef = (float *)malloc(nBytes);\n","    float *gpuRef  = (float *)malloc(nBytes);\n","\n","    //  initialize host array\n","    initialData(h_A, nrows * ncols);\n","\n","    //  transpose at host side\n","    transposeHost(hostRef, h_A, nrows, ncols);\n","\n","    // allocate device memory\n","    float *d_A, *d_C;\n","    CHECK(cudaMalloc((float**)&d_A, nBytes));\n","    CHECK(cudaMalloc((float**)&d_C, nBytes));\n","\n","    // copy data from host to device\n","    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n","\n","    // tranpose gmem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    double iStart = seconds();\n","    copyGmem<<<grid, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    double iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, nrows * ncols);\n","\n","    float ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) /\n","        iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"copyGmem elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> \"\n","           \"effective bandwidth %f GB\\n\", iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","\n","    // tranpose gmem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    naiveGmem<<<grid, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"naiveGmem elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> \"\n","           \"effective bandwidth %f GB\\n\", iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","\n","    // tranpose smem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    naiveGmemUnroll<<<grid2, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"naiveGmemUnroll elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> \"\n","           \"effective bandwidth %f GB\\n\", iElaps, grid2.x, grid2.y, block.x,\n","           block.y, ibnd);\n","\n","    // tranpose smem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmem<<<grid, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmem elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> \"\n","           \"effective bandwidth %f GB\\n\", iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","\n","    // tranpose smem pad\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmemPad<<<grid, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmemPad elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> \"\n","           \"effective bandwidth %f GB\\n\", iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","\n","    // tranpose smem pad\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmemDyn<<<grid, block, BDIMX*BDIMY*sizeof(float)>>>(d_C, d_A, nrows,\n","            ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmemDyn elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> \"\n","           \"effective bandwidth %f GB\\n\", iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","\n","    // tranpose smem pad\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmemPadDyn<<<grid, block, (BDIMX + IPAD) * BDIMY * sizeof(float)>>>(\n","          d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmemPadDyn elapsed %f sec <<< grid (%d,%d) block \"\n","           \"(%d,%d)>>> effective bandwidth %f GB\\n\", iElaps, grid.x, grid.y,\n","           block.x, block.y, ibnd);\n","\n","    // tranpose smem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmemUnroll<<<grid2, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmemUnroll elapsed %f sec <<< grid (%d,%d) block \"\n","           \"(%d,%d)>>> effective bandwidth %f GB\\n\", iElaps, grid2.x, grid2.y,\n","           block.x, block.y, ibnd);\n","\n","    // tranpose smem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmemUnrollPad<<<grid2, block>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmemUnrollPad elapsed %f sec <<< grid (%d,%d) block \"\n","           \"(%d,%d)>>> effective bandwidth %f GB\\n\", iElaps, grid2.x, grid2.y,\n","           block.x, block.y, ibnd);\n","\n","    // tranpose smem\n","    CHECK(cudaMemset(d_C, 0, nBytes));\n","    memset(gpuRef, 0, nBytes);\n","\n","    iStart = seconds();\n","    transposeSmemUnrollPadDyn<<<grid2, block, (BDIMX * 2 + IPAD) * BDIMY *\n","        sizeof(float)>>>(d_C, d_A, nrows, ncols);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","    if(iprint) printData(gpuRef, ncells);\n","\n","    checkResult(hostRef, gpuRef, ncols, nrows);\n","    ibnd = 2 * ncells * sizeof(float) / (1024.0 * 1024.0 * 1024.0) / iElaps;\n","    ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","    printf(\"transposeSmemUnrollPadDyn elapsed %f sec <<< grid (%d,%d) block \"\n","           \"(%d,%d)>>> effective bandwidth %f GB\\n\", iElaps, grid2.x, grid2.y,\n","           block.x, block.y, ibnd);\n","\n","    // free host and device memory\n","    CHECK(cudaFree(d_A));\n","    CHECK(cudaFree(d_C));\n","    free(h_A);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","    return EXIT_SUCCESS;\n","}\n"]},{"cell_type":"code","source":["%%cu\n","\n","\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","\n","#include <cuda.h>\n","#include <sys/time.h>\n","#ifndef _COMMON_H\n","#define _COMMON_H\n","\n","#define CHECK(call)                                                            \\\n","{                                                                              \\\n","    const cudaError_t error = call;                                            \\\n","    if (error != cudaSuccess)                                                  \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n","        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n","                cudaGetErrorString(error));                                    \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUBLAS(call)                                                     \\\n","{                                                                              \\\n","    cublasStatus_t err;                                                        \\\n","    if ((err = (call)) != CUBLAS_STATUS_SUCCESS)                               \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CUBLAS error %d at %s:%d\\n\", err, __FILE__,       \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CURAND(call)                                                     \\\n","{                                                                              \\\n","    curandStatus_t err;                                                        \\\n","    if ((err = (call)) != CURAND_STATUS_SUCCESS)                               \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CURAND error %d at %s:%d\\n\", err, __FILE__,       \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUFFT(call)                                                      \\\n","{                                                                              \\\n","    cufftResult err;                                                           \\\n","    if ( (err = (call)) != CUFFT_SUCCESS)                                      \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CUFFT error %d at %s:%d\\n\", err, __FILE__,        \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUSPARSE(call)                                                   \\\n","{                                                                              \\\n","    cusparseStatus_t err;                                                      \\\n","    if ((err = (call)) != CUSPARSE_STATUS_SUCCESS)                             \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got error %d at %s:%d\\n\", err, __FILE__, __LINE__);   \\\n","        cudaError_t cuda_err = cudaGetLastError();                             \\\n","        if (cuda_err != cudaSuccess)                                           \\\n","        {                                                                      \\\n","            fprintf(stderr, \"  CUDA error \\\"%s\\\" also detected\\n\",             \\\n","                    cudaGetErrorString(cuda_err));                             \\\n","        }                                                                      \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","inline double seconds()\n","{\n","    struct timeval tp;\n","    struct timezone tzp;\n","    int i = gettimeofday(&tp, &tzp);\n","    return ((double)tp.tv_sec + (double)tp.tv_usec * 1.e-6);\n","}\n","\n","#endif // _COMMON_H\n","\n","\n","/*\n"," * Various memory access pattern optimizations applied to a matrix transpose\n"," * kernel.\n"," */\n","\n","#define BDIMX 16\n","#define BDIMY 16\n","\n","void initialData(float *in,  const int size)\n","{\n","    for (int i = 0; i < size; i++)\n","    {\n","        in[i] = (float)( rand() & 0xFF ) / 10.0f; //100.0f;\n","    }\n","\n","    return;\n","}\n","\n","void printData(float *in,  const int size)\n","{\n","    for (int i = 0; i < size; i++)\n","    {\n","        printf(\"%dth element: %f\\n\", i, in[i]);\n","    }\n","\n","    return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int size, int showme)\n","{\n","    double epsilon = 1.0E-8;\n","    bool match = 1;\n","\n","    for (int i = 0; i < size; i++)\n","    {\n","        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n","        {\n","            match = 0;\n","            printf(\"different on %dth element: host %f gpu %f\\n\", i, hostRef[i],\n","                    gpuRef[i]);\n","            break;\n","        }\n","\n","        if (showme && i > size / 2 && i < size / 2 + 5)\n","        {\n","            // printf(\"%dth element: host %f gpu %f\\n\",i,hostRef[i],gpuRef[i]);\n","        }\n","    }\n","\n","    if (!match)  printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","void transposeHost(float *out, float *in, const int nx, const int ny)\n","{\n","    for( int iy = 0; iy < ny; ++iy)\n","    {\n","        for( int ix = 0; ix < nx; ++ix)\n","        {\n","            out[ix * ny + iy] = in[iy * nx + ix];\n","        }\n","    }\n","}\n","\n","__global__ void warmup(float *out, float *in, const int nx, const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[iy * nx + ix];\n","    }\n","}\n","\n","// case 0 copy kernel: access data in rows\n","__global__ void copyRow(float *out, float *in, const int nx, const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[iy * nx + ix];\n","    }\n","}\n","\n","// case 1 copy kernel: access data in columns\n","__global__ void copyCol(float *out, float *in, const int nx, const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[ix * ny + iy] = in[ix * ny + iy];\n","    }\n","}\n","\n","// case 2 transpose kernel: read in rows and write in columns\n","__global__ void transposeNaiveRow(float *out, float *in, const int nx,\n","                                  const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[ix * ny + iy] = in[iy * nx + ix];\n","    }\n","}\n","\n","// case 3 transpose kernel: read in columns and write in rows\n","__global__ void transposeNaiveCol(float *out, float *in, const int nx,\n","                                  const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[ix * ny + iy];\n","    }\n","}\n","\n","// case 4 transpose kernel: read in rows and write in columns + unroll 4 blocks\n","__global__ void transposeUnroll4Row(float *out, float *in, const int nx,\n","                                    const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x * 4 + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    unsigned int ti = iy * nx + ix; // access in rows\n","    unsigned int to = ix * ny + iy; // access in columns\n","\n","    if (ix + 3 * blockDim.x < nx && iy < ny)\n","    {\n","        out[to]                   = in[ti];\n","        out[to + ny * blockDim.x]   = in[ti + blockDim.x];\n","        out[to + ny * 2 * blockDim.x] = in[ti + 2 * blockDim.x];\n","        out[to + ny * 3 * blockDim.x] = in[ti + 3 * blockDim.x];\n","    }\n","}\n","\n","// case 5 transpose kernel: read in columns and write in rows + unroll 4 blocks\n","__global__ void transposeUnroll4Col(float *out, float *in, const int nx,\n","                                    const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x * 4 + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    unsigned int ti = iy * nx + ix; // access in rows\n","    unsigned int to = ix * ny + iy; // access in columns\n","\n","    if (ix + 3 * blockDim.x < nx && iy < ny)\n","    {\n","        out[ti]                = in[to];\n","        out[ti +   blockDim.x] = in[to +   blockDim.x * ny];\n","        out[ti + 2 * blockDim.x] = in[to + 2 * blockDim.x * ny];\n","        out[ti + 3 * blockDim.x] = in[to + 3 * blockDim.x * ny];\n","    }\n","}\n","\n","/*\n"," * case 6 :  transpose kernel: read in rows and write in colunms + diagonal\n"," * coordinate transform\n"," */\n","__global__ void transposeDiagonalRow(float *out, float *in, const int nx,\n","                                     const int ny)\n","{\n","    unsigned int blk_y = blockIdx.x;\n","    unsigned int blk_x = (blockIdx.x + blockIdx.y) % gridDim.x;\n","\n","    unsigned int ix = blockDim.x * blk_x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blk_y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[ix * ny + iy] = in[iy * nx + ix];\n","    }\n","}\n","\n","/*\n"," * case 7 :  transpose kernel: read in columns and write in row + diagonal\n"," * coordinate transform.\n"," */\n","__global__ void transposeDiagonalCol(float *out, float *in, const int nx,\n","                                     const int ny)\n","{\n","    unsigned int blk_y = blockIdx.x;\n","    unsigned int blk_x = (blockIdx.x + blockIdx.y) % gridDim.x;\n","\n","    unsigned int ix = blockDim.x * blk_x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blk_y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[ix * ny + iy];\n","    }\n","}\n","\n","// main functions\n","int main(int argc, char **argv)\n","{\n","    // set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"%s starting transpose at \", argv[0]);\n","    printf(\"device %d: %s \", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","\n","    // set up array size 2048\n","    int nx = 1 << 11;\n","    int ny = 1 << 11;\n","\n","    // select a kernel and block size\n","    int iKernel = 0;\n","    int blockx = 16;\n","    int blocky = 16;\n","\n","    if (argc > 1) iKernel = atoi(argv[1]);\n","\n","    if (argc > 2) blockx  = atoi(argv[2]);\n","\n","    if (argc > 3) blocky  = atoi(argv[3]);\n","\n","    if (argc > 4) nx  = atoi(argv[4]);\n","\n","    if (argc > 5) ny  = atoi(argv[5]);\n","\n","    printf(\" with matrix nx %d ny %d with kernel %d\\n\", nx, ny, iKernel);\n","    size_t nBytes = nx * ny * sizeof(float);\n","\n","    // execution configuration\n","    dim3 block (blockx, blocky);\n","    dim3 grid  ((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","    // allocate host memory\n","    float *h_A = (float *)malloc(nBytes);\n","    float *hostRef = (float *)malloc(nBytes);\n","    float *gpuRef  = (float *)malloc(nBytes);\n","\n","    // initialize host array\n","    initialData(h_A, nx * ny);\n","\n","    // transpose at host side\n","    transposeHost(hostRef, h_A, nx, ny);\n","\n","    // allocate device memory\n","    float *d_A, *d_C;\n","    CHECK(cudaMalloc((float**)&d_A, nBytes));\n","    CHECK(cudaMalloc((float**)&d_C, nBytes));\n","\n","    // copy data from host to device\n","    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n","\n","    // warmup to avoide startup overhead\n","    double iStart = seconds();\n","    warmup<<<grid, block>>>(d_C, d_A, nx, ny);\n","    CHECK(cudaDeviceSynchronize());\n","    double iElaps = seconds() - iStart;\n","    printf(\"warmup         elapsed %f sec\\n\", iElaps);\n","    CHECK(cudaGetLastError());\n","\n","    // kernel pointer and descriptor\n","    void (*kernel)(float *, float *, int, int);\n","    char *kernelName;\n","\n","    // set up kernel\n","    switch (iKernel)\n","    {\n","    case 0:\n","        kernel = &copyRow;\n","        kernelName = \"CopyRow       \";\n","        break;\n","\n","    case 1:\n","        kernel = &copyCol;\n","        kernelName = \"CopyCol       \";\n","        break;\n","\n","    case 2:\n","        kernel = &transposeNaiveRow;\n","        kernelName = \"NaiveRow      \";\n","        break;\n","\n","    case 3:\n","        kernel = &transposeNaiveCol;\n","        kernelName = \"NaiveCol      \";\n","        break;\n","\n","    case 4:\n","        kernel = &transposeUnroll4Row;\n","        kernelName = \"Unroll4Row    \";\n","        grid.x = (nx + block.x * 4 - 1) / (block.x * 4);\n","        break;\n","\n","    case 5:\n","        kernel = &transposeUnroll4Col;\n","        kernelName = \"Unroll4Col    \";\n","        grid.x = (nx + block.x * 4 - 1) / (block.x * 4);\n","        break;\n","\n","    case 6:\n","        kernel = &transposeDiagonalRow;\n","        kernelName = \"DiagonalRow   \";\n","        break;\n","\n","    case 7:\n","        kernel = &transposeDiagonalCol;\n","        kernelName = \"DiagonalCol   \";\n","        break;\n","    }\n","\n","    // run kernel\n","    iStart = seconds();\n","    kernel<<<grid, block>>>(d_C, d_A, nx, ny);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    // calculate effective_bandwidth\n","    float ibnd = 2 * nx * ny * sizeof(float) / 1e9 / iElaps;\n","    printf(\"%s elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> effective \"\n","           \"bandwidth %f GB\\n\", kernelName, iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","    CHECK(cudaGetLastError());\n","\n","    // check kernel results\n","    if (iKernel > 1)\n","    {\n","        CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","        checkResult(hostRef, gpuRef, nx * ny, 1);\n","    }\n","\n","    // free host and device memory\n","    CHECK(cudaFree(d_A));\n","    CHECK(cudaFree(d_C));\n","    free(h_A);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","    return EXIT_SUCCESS;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jdqvBOrtCB7","executionInfo":{"status":"ok","timestamp":1701356328268,"user_tz":-330,"elapsed":1644,"user":{"displayName":"Venkatesh Elangovan","userId":"12247996726828990634"}},"outputId":"24b78fd4-a264-40be-ef72-3fbfe9e9bc5a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/tmp/tmp4_ksjuqb/ede1fb1f-73cc-482a-bad8-beb095f261b4.out starting transpose at device 0: Tesla T4  with matrix nx 2048 ny 2048 with kernel 0\n","warmup         elapsed 0.000245 sec\n","CopyRow        elapsed 0.000172 sec <<< grid (128,128) block (16,16)>>> effective bandwidth 194.927277 GB\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eJ-jLG5Vtxat"},"execution_count":null,"outputs":[]}]}